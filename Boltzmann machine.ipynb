{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boltzmann machine\n",
    "\n",
    "## A physical introduction\n",
    "\n",
    "A Boltzmann machine is a _symmetric binary stochastic neural network_. This means that the nodes (also called neurons) of the network can only assume one of two possible states, namely 1 and 0 or, equivalently, \"on\" and \"off\" (__binary property__). At each iteration step, the state of such a neuron is switched on (or kept on) with a certain __probability p__ (__stochastic property__). Intuitively, a neuron should more likely to be active (i.e. in the \"on\" state) if it receives a lot of input. This is exactly what is the case in the Boltzmann machine, where the probability $p(s_i)$ that neuron $i$ is in state $s=1^{[1],[2]}$:\n",
    "\n",
    "$$p(s_i=1) = \\frac{1}{1+e^{-\\frac{z_i}{T}}},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$z_i = b_i + \\sum_{j}^N s_j\\omega_{ij}, \\,\\,\\, \\omega_{ii} = 0.$$\n",
    "\n",
    "$z_i$ is the input that unit $i$ receives (where bias acts as input as well). Here, $s_j$ is the state of neuron $j$, $N$ is the number of neurons in the network, $b_i$ is the _bias_ of neuron $i$ and $\\omega_{ij}$ is the connection strength from neuron $j$ to neuron $i$. Importantly connections are symmetric, i.e. $\\omega_{ij}=\\omega_{ji}$ (__symmetric property__). $T$ is the system temperature. It determines the slope of the sigmoid function $p(s_i = 1)$, which turns into the Heaviside stepfunction for $T \\rightarrow 0$:\n",
    "\n",
    "<center><img  src=\"sigmoid.png\"> Fig. 1: Sigmoid function for different system temperatures.</center>\n",
    "\n",
    "The state of the network is completely described by state vector $\\vec{s}=(s_1, s_2, ..., s_n)^T$, $s_i\\in{0,1}$.\n",
    "\n",
    "Before coming to the eponymous feature of the Boltzmann machine, we first have to introduce the concept of the _energy_ of the network. Each state $\\vec{s}$ is associated with a scalar function called the state's _energy_ E($\\vec{s}$):\n",
    "\n",
    "$$E(\\vec{s}) = -\\sum_i s_i b_i - \\sum_{i \\lt j} s_is_j\\omega_{ij},$$\n",
    "where\n",
    "$$\\sum_{i \\lt j} s_is_j\\omega_{ij} \\doteq  \\sum_{i=1}^j \\sum_{j=2}^N s_is_j\\omega_{ij},$$\n",
    "\n",
    "The energy of a state is thus __the negative sum of the biases of all active neurons minus the sum of connection weights of connections whose neurons on both ends are active__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Some words about the energy function</b>\n",
    "\n",
    "Why is $E(\\vec{s})$ called the energy? It's called energy because it behaves like a physical energy: it's a scalar function whcih the system tends to minimize. How can we see that the energy is actually minimized by the system? To understand this, it is helpful to realise that the input $z_i$ to a neuron is is equal to the amount by which $E(\\vec{s})$ is reduced, when unit $i$ turns from off to on:\n",
    "\n",
    "$$E(s_i = 0)-E(s_i=1) = z_i.$$\n",
    "\n",
    "Now consider the case $T\\rightarrow 0$, for which the probability of a neuron being on turns into a step function (Fig. 1): If $z_i>0$ (which means that a switching on of the state would reduce the energy function), the state is set to \"on\" with 100% certainty. If $z_i<0$ (which means that a switching on of the state would increase the energy function), the state is set to \"off\" with certainty. If $T<<\\infty$, then this behaviour is not deterministic anymore, but as $z_i>0$ still means that state $i$ will be on most of the time ($z_i > 0 \\rightarrow p(z_i)>0.5$), the system _tends_ to minimize its energy function.\n",
    "\n",
    "It is however hard, if not impossible, to easily say what configuration the system takes to assume. In the simple case where all biases are greater or equal to zero, and all connection weights are larger than zero, it is easy to see that a minimization of the energy is equivalent to a turning on of all neurons. Similarly the system tends to turn neurons off if biases and weights are all negative. In the case however where biases and/ or weights can be both negative and positive, the energy-minimizing configuration is not that easy to see.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the neuronal states of the network are updated in a way that is not dependent on the states, the network will eventually settle in a __Boltzmann distribution__. This means, that the occurrence probability $p(\\vec{s})$ of a state vector $\\vec{s}$ is dependent only to its energy:\n",
    "\n",
    "$$p(\\vec{s}) = \\frac{e^{-E(\\vec{s})}}{\\sum_\\vec{u}e^{-E(\\vec{u})}}$$\n",
    "\n",
    "where $\\sum_\\vec{u}$ is the sum over all possible network states. Note that $p(\\vec{s})$ is proportional to the exponential of the state's negative energy, as $\\sum_\\vec{u}e^{-E(\\vec{u})} = const$. The distribution for $p(\\vec{s})$ derives its name from the similar-looking [Boltzmann Distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution) from statistical mechanics, which describes the probability of occurrence of a physical state in a thermodynamical system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] [Geoffrey E. Hinton (2007) Boltzmann machine. Scholarpedia, 2(5):1668.](http://www.scholarpedia.org/article/Boltzmann_machine)\n",
    "\n",
    "[2] [Wikipedia: Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
