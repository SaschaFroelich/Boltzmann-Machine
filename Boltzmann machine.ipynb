{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boltzmann machine\n",
    "\n",
    "## Introduction: Neuroscience or Computer Science?\n",
    "\n",
    "### Neuroscience?\n",
    "\n",
    "The Boltzmann machine is interesting from many perspectives and has found applications in computer science, neuroscience and the cognitive sciences. It is suggested that the basic principles governing the Boltzmann machine may be similar to the working principles of networks of biological neurons, such as the human brain. Every neurobiologist will flinch when they hear the brain compared to the Boltzmann machine, as there are many reasons why the Boltzmann machine, strictly speaking, is quite different from a real brain. We will cover these points later. The BM is however quite interesting for neuroscience as it learns in an unsupervised manner bassed on simple learning rules that may (or may not) be similar to principles of synaptic plasticity observed in real brains (see \"Hebbian learning\" below). The most important aspect of BM learning is the fact that it learns, in an unsupervised manner, the __statistical regularities__ of its data, which is something that the brain is thought to do as well. In fact, the learning of statistical regularities is the only sensible way of describing learning in the brain. For example, we have now innate sense for the fact that objects will always fall to the ground. We learn this fact from observation and the fact that no object ever falls upwards by itself, and it has been shown that even infants learn from statistical correlations (Check out [this](https://www.ted.com/talks/laura_schulz_the_surprisingly_logical_minds_of_babies) amazing TED talk, for example). Furthermore, the BM is a __generative model__: Once it has been trained on a data set, it can let to evolve freely and will produce states that satisfy the statistical regularities of the data it was trained on. For example, check out this randomly produced pattern that my BM produced after it was trained on the handwritten digits 1, 5 and 8:\n",
    "\n",
    "![Randomly generated pattern](.image91.png)\n",
    "\n",
    "It is a sensible assumption that any statistical model that wants to describe the functioning of the brain has to be a generative one. For example, when someone talks to you in English, you understand what they say, but you can at the same time produce sentences that are coherent and adhere to the rules of the English language. This is what a generative model does. A __discriminative model__ is only capable of __data classification__, but not of __data generation__. A simple example of a purely discriminative model is a deep neural network. This is one of the reasons why many theoretical neuroscientists believe that vanilla deep learning cannot completely describe the functioning of the brain. <br>\n",
    "\n",
    "So, the Boltzmann Machine is a good first candidate for a model of real biological networks. As a fun sidenote: The training algorithm for a Boltzmann Machine requires periods of data intake, to learn the statistical regularities of the data, and periods of \"free association\" where no data is taken in, which serves to estimate the internalized statistics to compare them to the statistics in the input data. Doesn't that sound a lot like a wake-sleep cycle? Some scientists even started modelling psychiatric/ neurological phenomena with Boltzmann machines [5].\n",
    "\n",
    "There are however a number of reasons why the BM may not be the end of the story, either. One obvious reason is that the connections between neurons in a Boltzmann Machine have to always be symmetrical, which is not at all true in the human brain: While there is (almost) always reciprocity between neurons in the brain, these connections are usually rather asymmetric [6,7].\n",
    "\n",
    "### Computer Science?\n",
    "\n",
    "The Boltzmann Machine is painstakingly slow to train. More to follow...\n",
    "\n",
    "## Example Walkthrough\n",
    "\n",
    "After all this talk, let's look at a Boltzmann Machine, but for a start, without the math! (To be continued)\n",
    "\n",
    "## The math\n",
    "\n",
    "A Boltzmann machine is a _symmetric binary stochastic neural network_. This means that the nodes (also called neurons) of the network can only assume one of two possible states, namely 1 and 0 or, equivalently, \"on\" and \"off\" (__binary property__). At each iteration step, the state of such a neuron is switched on (or kept on) with a certain __probability p__ (__stochastic property__). Intuitively, a neuron should more likely to be active (i.e. in the \"on\" state) if it receives a lot of input. This is exactly what is the case in the Boltzmann machine, where the probability $p(s_i)$ that neuron $i$ is in state $s=1^{[1],[2]}$:\n",
    "\n",
    "$$p(s_i=1) = \\frac{1}{1+e^{-\\frac{z_i}{T}}},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$z_i = b_i + \\sum_{j}^N s_j\\omega_{ij}, \\,\\,\\, \\omega_{ii} = 0.$$\n",
    "\n",
    "$z_i$ is the input that unit $i$ receives (where bias acts as input as well). Here, $s_j$ is the state of neuron $j$, $N$ is the number of neurons in the network, $b_i$ is the _bias_ of neuron $i$ and $\\omega_{ij}$ is the connection strength from neuron $j$ to neuron $i$. Importantly connections are symmetric, i.e. $\\omega_{ij}=\\omega_{ji}$ (__symmetric property__). $T$ is the system temperature. It determines the slope of the sigmoid function $p(s_i = 1)$, which turns into the Heaviside stepfunction for $T \\rightarrow 0$:\n",
    "\n",
    "![Activation function of unit i](./sigmoid.png)\n",
    "\n",
    "The state of the network is completely described by state vector $\\vec{s}=(s_1, s_2, ..., s_n)^T$, $s_i\\in{0,1}$.\n",
    "\n",
    "Before coming to the eponymous feature of the Boltzmann machine, we first have to introduce the concept of the _energy_ of the network. Each state $\\vec{s}$ is associated with a scalar function called the state's _energy_ E($\\vec{s}$):\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Energy of a state $\\vec{s}$</b>\n",
    "$$E(\\vec{s}) = -\\sum_i s_i b_i - \\sum_{i \\lt j} s_is_j\\omega_{ij}.$$\n",
    "</div>\n",
    "Here:\n",
    "$$\\sum_{i \\lt j} s_is_j\\omega_{ij} \\doteq  \\sum_{i=1}^j \\sum_{j=2}^N s_is_j\\omega_{ij},$$\n",
    "\n",
    "The energy of a state is thus __the negative sum of the biases of all active neurons minus the sum of connection weights of connections whose neurons on both ends are active__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Some words about the energy function</b>\n",
    "\n",
    "Why is $E(\\vec{s})$ called the energy? It's called energy because it behaves like a physical energy: it's a scalar function whcih the system tends to minimize. How can we see that the energy is actually minimized by the system? To understand this, it is helpful to realise that the input $z_i$ to a neuron is is equal to the amount by which $E(\\vec{s})$ is reduced, when unit $i$ turns from off to on:\n",
    "\n",
    "$$E(s_i = 0)-E(s_i=1) = z_i.$$\n",
    "\n",
    "Now consider the case $T\\rightarrow 0$, for which the probability of a neuron being on turns into a step function (Fig. 1): If $z_i>0$ (which means that a switching on of the state would reduce the energy function), the state is set to \"on\" with 100% certainty. If $z_i<0$ (which means that a switching on of the state would increase the energy function), the state is set to \"off\" with certainty. If $T<<\\infty$, then this behaviour is not deterministic anymore, but as $z_i>0$ still means that state $i$ will be on most of the time ($z_i > 0 \\rightarrow p(z_i)>0.5$), the system _tends_ to minimize its energy function.\n",
    "\n",
    "It is however hard, if not impossible, to easily say what configuration the system takes to assume. In the simple case where all biases are greater or equal to zero, and all connection weights are larger than zero, it is easy to see that a minimization of the energy is equivalent to a turning on of all neurons. Similarly the system tends to turn neurons off if biases and weights are all negative. In the case however where biases and/ or weights can be both negative and positive, the energy-minimizing configuration is not that easy to see.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the neuronal states of the network are updated in a way that is not dependent on the states, the network will eventually settle in a __Boltzmann distribution__. This means, that the occurrence probability $p(\\vec{s})$ of a state vector $\\vec{s}$ is dependent only to its energy:\n",
    "\n",
    "$$p(\\vec{s}) = \\frac{e^{-E(\\vec{s})}}{\\sum_\\vec{u}e^{-E(\\vec{u})}} \\ \\ \\ \\ \\ \\ (1)$$\n",
    "\n",
    "where $\\sum_\\vec{u}$ is the sum over all possible network states. Note that $p(\\vec{s})$ is proportional to the exponential of the state's negative energy, as $\\sum_\\vec{u}e^{-E(\\vec{u})} = const$. The distribution for $p(\\vec{s})$ derives its name from the similar-looking [Boltzmann Distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution) from statistical mechanics, which describes the probability of occurrence of a physical state in a thermodynamical system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "The learning rules for the connection strengths and biases of a Boltzmann machine without hidden units are$^{[1]}$:\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Learning of a Boltzmann Machine</b>\n",
    "$$\\Delta \\omega_{ij} = \\alpha_\\omega \\cdot \\left( \\langle s_i s_j \\rangle_{data} - \\langle s_i s_j \\rangle_{model} \\right)$$\n",
    "$$\\Delta b_i = \\alpha_b \\cdot \\left( \\langle s_j \\rangle_{data} - \\langle s_j \\rangle_{model} \\right)$$\n",
    "    </div>\n",
    "    \n",
    "Here $\\alpha_\\omega$ and $\\alpha_b$ are the learning rates, and _model_ stands for the Boltzmann machine. Expect for very simple networks, the value $\\langle s_i s_j \\rangle_{model}$ is not easily accessible. To compute its exact value, we would have to evaluate equation 1 (the \"Boltzmann probability\" of a state), where we sum over all possible state vectors in the denominator. As this number scales exponentially with the number of units in the Boltzmann machine, this quickly becomes intractable. For example, while there are $2^{10} = 1024$ different possible states with 10 units, for a mere 50 units, there are $2^{50} = 1,125,899,906,842,624$ different states. Therefore, the value $\\langle s_i s_j \\rangle_{model}$ is commonly approximated by letting the BM run for a while and counting the occurrences of the different state vectors.\n",
    "\n",
    "## Hebbian Learning\n",
    "\n",
    "The above learning rules are interesting not only for their simplicity, but also for the fact that they are reminiscent to the neurobiological learning rule of [_Hebbian Learning_](https://en.wikipedia.org/wiki/Hebbian_theory#Hebbian_learning_account_of_mirror_neurons). Hebbian Learning states, somewhat simplified: \"What fires together, wires together\" and means that neurons that tend to fire simultaneously will form connections to excite each other. While this certainly has its validity for artificial neural networks, it turns out that it is not quite so simple for biological neurons. Many neuroscientific experiments have established that Hebbian learning requires temporal asynchrony, i.e. neuron A has to consistenly fire before neuron B in order for the connection from A to B to get strengthened over time$^{[4]}$. This well-established concept in neuroscience is now known as [Spike-Timing Dependent Plasticity (STDP)](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] [Geoffrey E. Hinton (2007) Boltzmann machine. Scholarpedia, 2(5):1668.](http://www.scholarpedia.org/article/Boltzmann_machine)\n",
    "\n",
    "[2] [Wikipedia: Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine)\n",
    "\n",
    "[3] Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. \n",
    "Cognitive science, 9(1), 147-169.\n",
    "\n",
    "[4] [Spike-Timing Dependent Plasticity (STDP) (Wikipedia)](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)\n",
    "\n",
    "[5] [Series, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model. In Advances in Neural Information Processing Systems (pp. 2020-2028).](https://homepages.inf.ed.ac.uk/pseries/pdfs/ReichertSeriesStorkeyNIPS.pdf)\n",
    "\n",
    "[6] Zeki, S., & Shipp, S. (1988). The functional logic of cortical connections. Nature, 335(6188), 311-317.\n",
    "\n",
    "[7] Sherman, S. M., & Guillery, R. W. (1998). On the actions that one nerve cell can have on another: distinguishing “drivers” from “modulators”. Proceedings of the National Academy of Sciences, 95(12), 7121-7126.\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Coincidence_detection_in_neurobiology\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Synaptic_tagging\n",
    "\n",
    "- https://www.mitpressjournals.org/doi/10.1162/neco.1995.7.5.889\n",
    "\n",
    "\n",
    "\n",
    "### Papers\n",
    "\n",
    "- [Hinton, G. E. (2012). A practical guide to training restricted Boltzmann machines. In Neural networks: Tricks of the trade (pp. 599-619). Springer, Berlin, Heidelberg.\n",
    "](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "\n",
    "- [Bengio, Y., Lee, D. H., Bornschein, J., Mesnard, T., & Lin, Z. (2015). Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156.](https://arxiv.org/pdf/1502.04156.pdf)\n",
    "\n",
    "- [Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine. Neural computation, 7(5), 889-904.](http://www.gatsby.ucl.ac.uk/~dayan/papers/hm95.pdf)\n",
    "\n",
    "- [Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558.](https://www.pnas.org/content/pnas/79/8/2554.full.pdf)\n",
    "\n",
    "- [Hinton, G. E., & Sejnowski, T. J. (1983, June). Optimal perceptual inference. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (Vol. 448). IEEE New York.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.445.4504&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Boltzmann Distribution\n",
    "\n",
    "Here I want to derive the fact that a Boltzmann machine creates a state $\\vec{s}$ with the Boltzmann-probability (equation 1).\n",
    "\n",
    "Remember the probability given that unit $i$ is turned on:\n",
    "\n",
    "$$p(s_i=1)=\\frac{1}{1+e^{-z_i}},$$\n",
    "\n",
    "and that $z_i$ is the energy contribution to the state of unit $i$ when turned on against being turned off, thus\n",
    "\n",
    "$$z_i = \\Delta E_i.$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$p(s_i=1)=\\frac{1}{1+e^{-\\Delta E_i}}.$$\n",
    "\n",
    "Now, imagine a Boltzmann machine with four units. We are interested in the state $\\vec{s}_{1,2}$ where units 1 and 2 are on, and units 3 and 4 are off. The probability of that state is given by the product of the different units being in the required state:\n",
    "\n",
    "$$p(\\vec{s}_{1,2})=\\frac{1}{1+e^{-\\Delta E_1}}\\cdot \\frac{1}{1+e^{-\\Delta E_2}} \\cdot \\left( 1- \\frac{1}{1+e^{-\\Delta E_3}}\\right)\\cdot \\left( 1- \\frac{1}{1+e^{-\\Delta E_4}}\\right).$$\n",
    "\n",
    "For the next step, we use the fact that $\\frac{1}{1+e^{-z_i}}=1-\\frac{1}{1+e^{z_i}}$, thus:\n",
    "\n",
    "$$p(\\vec{s}_{1,2})=\\left( 1- \\frac{1}{1+e^{\\Delta E_1}}\\right)\\cdot \\left( 1- \\frac{1}{1+e^{\\Delta E_2}}\\right)\\cdot \\frac{1}{1+e^{\\Delta E_3}}\\cdot \\frac{1}{1+e^{\\Delta E_4}}$$\n",
    "$$= \\frac{e^{\\Delta E_1}}{1+e^{\\Delta E_1}}\\cdot \\frac{e^{\\Delta E_2}}{1+e^{\\Delta E_2}}\\cdot \\frac{1}{1+e^{\\Delta E_3}}\\cdot \\frac{1}{1+e^{\\Delta E_4}}.$$\n",
    "$$=\\frac{e^{\\Delta E_1 + \\Delta E_2}}{\\prod_{i=1}^4 (1+e^{\\Delta E_i})}$$\n",
    "\n",
    "We're almost there. Remember that, when unit i is turned on, the system's energy _decreases_ by $\\Delta E_i$, thus $E=-\\sum_i E_i$. To see the identity of the denominator, we substitute $e^{\\Delta E_1} = a$, $e^{\\Delta E_2} = b$, $e^{\\Delta E_3} = c$, and $e^{\\Delta E_4} = d$. Then, the denominator is:\n",
    "\n",
    "$$(1+a)(1+b)(1+c)(1+d) = a b c d + a b c + a b d + a b + a c d + a c + a d + a + b c d + b c + b d + b + c d + c + d + 1.$$\n",
    "\n",
    "As you can see, the result contains every possible combination of products between a, b, c, and d of length four and less. Resusbtituting the first term of that line: $abcd = e^{\\Delta E_1+ \\Delta E_2+\\Delta E_3+\\Delta E_4}$, where $\\Delta E_1+ \\Delta E_2+\\Delta E_3+\\Delta E_4$ is the negative of the energy of the system when all four units are on. Thus, the denominator is a linear combination of the negative exponentials of the energies of all the possible states of the Boltzmann machine. The probably for the state we started with thus works out to be:\n",
    "\n",
    "$$p(\\vec{s}_{1,2}) = \\frac{e^{E(\\vec{s}_{1,2})}}{\\sum_\\vec{u} e^{-E(\\vec{u})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Learning Rules\n",
    "The learning rules can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle \\frac{\\partial}{\\partial \\omega_{ij}} \\ln p(\\vec{v})\\rangle_{data} = \\langle \\frac{\\partial}{\\partial \\omega_{ij}}\\left( -E(\\vec{v}) -\\ln \\sum_{\\vec{u}} e^{-E(\\vec{u})} \\right) \\rangle_{data} \\ \\ \\ \\ \\ \\ (2)\n",
    "\\end{align}\n",
    "$$\n",
    "Here, we used the identity of Equation (1) above. $\\langle \\cdot \\rangle_{data}$ stands for the expected value under the distribution of state vectors $\\vec{v}$ in the data. We can thus rewrite the above equation as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\vec{v}} p_{data}(\\vec{v})\\left( \\frac{\\partial}{\\partial \\omega_{ij}}\\left( -E(\\vec{v}) -\\ln \\sum_{\\vec{u}} e^{-E(\\vec{u})} \\right) \\right)  \\ \\ \\ \\ \\ \\ (3)\n",
    "\\end{align}\n",
    "$$\n",
    "In the previous equation $p_{data}(\\vec{v})$ stands for the probability of state vector $\\vec{v}$ occurring in the training data and must therefore not be confused with the boltzmann distribution in equation (1) above.\n",
    "\n",
    "For the next step we use the fact that $\\frac{\\partial}{\\partial \\omega_{ij}}E(\\vec{v})=-s_is_j$. At the same time we use $\\frac{\\partial}{\\partial \\omega_{ij}} \\ln \\sum_{\\vec{u}}e^{-E(\\vec{u})} = \\frac{\\frac{\\partial}{\\partial \\omega_{ij}} \\sum_{\\vec{u}}e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}$, because of $\\frac{\\partial}{\\partial x} \\ln(f(x)) = \\frac{\\frac{\\partial}{\\partial x} f(x)}{f(x)}$. Thus, equation (3) can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\vec{v}} p_{data}(\\vec{v})(s_is_j)_\\vec{v} - \\sum_{\\vec{v}} p_{data}(\\vec{v}) \\frac{\\frac{\\partial}{\\partial \\omega_{ij}} \\sum_{\\vec{u}}e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  \\ \\ \\ \\ \\ \\ (4).\n",
    "\\end{align}\n",
    "$$\n",
    "Now this is starting to look a little complicated, but upon closer inspection we see that the left hand side is simply the expectation value of $s_is_j$ under the state-distribution of the data, and the right-hand side is independent of the vector $\\vec{v}$ we started with. Therefore, the sum on the RHS just sums to one and equation (4) can be rewritten as (writing the derivative inside the sum):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle s_is_j \\rangle_{data} - \\frac{\\sum_{\\vec{u}} \\frac{\\partial}{\\partial \\omega_{ij}} e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  \\ \\ \\ \\ \\ \\ (5).\n",
    "\\end{align}\n",
    "$$\n",
    "Derivation as before gives:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle s_is_j \\rangle_{data} - \\frac{\\sum_{\\vec{u}} (s_is_j)_\\vec{u}\\cdot e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  = \\langle s_is_j \\rangle_{data} - \\sum_\\vec{u}\\left( p(\\vec{u})\\cdot(s_is_j)_\\vec{u}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last step we used the definition of the Boltzmann distribution from euqation (1). As the distribution describes the distribution of state vectors of the Boltzmann machine (which is the _model_), the LHS of equation resolves to:\n",
    "\n",
    "$$\\langle \\frac{\\partial}{\\partial \\omega_{ij}} \\ln p(\\vec{v})\\rangle_{data} = \\langle s_is_j \\rangle_{data} - \\langle s_is_j \\rangle_{model}.$$\n",
    "\n",
    "The learning rule for $b_i$ can be derived analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation to the Boltzmann Distribution\n",
    "\n",
    "We want to check that the Boltzmann machine (BM) actually generates state vectors with their respective boltzmann-probabilities (equation (1)). This should be the case after the BM had the chance to settle into its stable state, thus after a certain number of iterations. The longer it iterates (i.e. generates data), the closer it should be to the Boltzmann distribution.\n",
    "\n",
    "In the following we will check whether the average distance between the boltzmann-probabilities of binary states with 10 entries and their probabilities of being generated by the (randomly initialised) BM actually decreases with more iterations.\n",
    "\n",
    "Note: The following is merely a proof of principle and not a strict mathematical proof the boltzmann property!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iterate() got an unexpected keyword argument 'savehist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c638f53af888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoltzmannM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initialise BM (random weights and biases are automatically assigned)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlaststate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavehist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Iterate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iterate() got an unexpected keyword argument 'savehist'"
     ]
    }
   ],
   "source": [
    "import BM #import Boltzmann Machine script (from this repository)\n",
    "import numpy as np\n",
    "N = 10 # There are 1024 different binary state vectors of length 10\n",
    "\n",
    "iterations = [i**3 for i in range(1,25)]\n",
    "distances = []\n",
    "\n",
    "for no_of_iterations in iterations:\n",
    "    bm = BM.BoltzmannM(N, initial_state=np.ones(N)) # Initialise BM (random weights and biases are automatically assigned)\n",
    "    \n",
    "    laststate, state_history = bm.iterate(no_of_iterations, savehist=True, suppress_output=True) # Iterate \n",
    "\n",
    "    import itertools\n",
    "    all_possible_states = [list(i) for i in itertools.product([0, 1], repeat=N)] # Create a list of all 1024 possible states\n",
    "    energies = bm.energy(all_possible_states) # Use the method energy() to compute the energy for each element in the list (Here an element is a list of length 10)\n",
    "\n",
    "    boltzmann_probs = []\n",
    "    for idx, state in enumerate(all_possible_states):\n",
    "        boltzmann_probability = np.exp(-energies[idx])/np.exp(-np.array(energies)).sum()\n",
    "        boltzmann_probs.append(boltzmann_probability)\n",
    "\n",
    "    # boltzmann_probs now contains the boltzmann-probabilities for the states in all_possible_states\n",
    "    # Compute the probability of a state actually being generated by the model\n",
    "\n",
    "    model_probs = []\n",
    "    for state in all_possible_states:\n",
    "        occurences_in_history = [1 for s in state_history if s == state]\n",
    "        model_probs.append(sum(occurences_in_history)/no_of_iterations)\n",
    "\n",
    "    # model_probs now contains the probabilities of occurrence for the states in state_history (generated by the BM)\n",
    "\n",
    "    # The average distance between the probabilities is thus \n",
    "    average_distance = (abs(np.array(boltzmann_probs) - np.array(model_probs))).mean()\n",
    "    distances.append(average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (24,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b588637f5698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<p(state)$_{BM}$ - p(state)$_{boltzmann}$>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (24,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(iterations,distances)\n",
    "plt.title('<p(state)$_{BM}$ - p(state)$_{boltzmann}$>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "- Implement learning with hidden units\n",
    "\n",
    "- Bayesian properties?\n",
    "\n",
    "- Notebook: Hopfield networks\n",
    "\n",
    "- Read: Completion of incomplete data\n",
    "\n",
    "- Notebook: compare with Kennet (2003)\n",
    "\n",
    "- Restricted Boltzmann machines\n",
    "\n",
    "- What does this have to do with Helmholtz machine?\n",
    "\n",
    "- Relationship: Training algorithm vs wake-sleep algorithm\n",
    "\n",
    "\n",
    "# To Solve\n",
    "\n",
    "- (On-line learning algorithms?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
