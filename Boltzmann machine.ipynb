{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boltzmann machine\n",
    "\n",
    "## A short introduction\n",
    "\n",
    "A Boltzmann machine is a _symmetric binary stochastic neural network_. This means that the nodes (also called neurons) of the network can only assume one of two possible states, namely 1 and 0 or, equivalently, \"on\" and \"off\" (__binary property__). At each iteration step, the state of such a neuron is switched on (or kept on) with a certain __probability p__ (__stochastic property__). Intuitively, a neuron should more likely to be active (i.e. in the \"on\" state) if it receives a lot of input. This is exactly what is the case in the Boltzmann machine, where the probability $p(s_i)$ that neuron $i$ is in state $s=1^{[1],[2]}$:\n",
    "\n",
    "$$p(s_i=1) = \\frac{1}{1+e^{-\\frac{z_i}{T}}},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$z_i = b_i + \\sum_{j}^N s_j\\omega_{ij}, \\,\\,\\, \\omega_{ii} = 0.$$\n",
    "\n",
    "$z_i$ is the input that unit $i$ receives (where bias acts as input as well). Here, $s_j$ is the state of neuron $j$, $N$ is the number of neurons in the network, $b_i$ is the _bias_ of neuron $i$ and $\\omega_{ij}$ is the connection strength from neuron $j$ to neuron $i$. Importantly connections are symmetric, i.e. $\\omega_{ij}=\\omega_{ji}$ (__symmetric property__). $T$ is the system temperature. It determines the slope of the sigmoid function $p(s_i = 1)$, which turns into the Heaviside stepfunction for $T \\rightarrow 0$:\n",
    "\n",
    "![Activation function of unit i](./sigmoid.png)\n",
    "\n",
    "The state of the network is completely described by state vector $\\vec{s}=(s_1, s_2, ..., s_n)^T$, $s_i\\in{0,1}$.\n",
    "\n",
    "Before coming to the eponymous feature of the Boltzmann machine, we first have to introduce the concept of the _energy_ of the network. Each state $\\vec{s}$ is associated with a scalar function called the state's _energy_ E($\\vec{s}$):\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Energy of a state $\\vec{s}$</b>\n",
    "$$E(\\vec{s}) = -\\sum_i s_i b_i - \\sum_{i \\lt j} s_is_j\\omega_{ij}.$$\n",
    "</div>\n",
    "Here\n",
    "$$\\sum_{i \\lt j} s_is_j\\omega_{ij} \\doteq  \\sum_{i=1}^j \\sum_{j=2}^N s_is_j\\omega_{ij},$$\n",
    "\n",
    "The energy of a state is thus __the negative sum of the biases of all active neurons minus the sum of connection weights of connections whose neurons on both ends are active__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Some words about the energy function</b>\n",
    "\n",
    "Why is $E(\\vec{s})$ called the energy? It's called energy because it behaves like a physical energy: it's a scalar function whcih the system tends to minimize. How can we see that the energy is actually minimized by the system? To understand this, it is helpful to realise that the input $z_i$ to a neuron is is equal to the amount by which $E(\\vec{s})$ is reduced, when unit $i$ turns from off to on:\n",
    "\n",
    "$$E(s_i = 0)-E(s_i=1) = z_i.$$\n",
    "\n",
    "Now consider the case $T\\rightarrow 0$, for which the probability of a neuron being on turns into a step function (Fig. 1): If $z_i>0$ (which means that a switching on of the state would reduce the energy function), the state is set to \"on\" with 100% certainty. If $z_i<0$ (which means that a switching on of the state would increase the energy function), the state is set to \"off\" with certainty. If $T<<\\infty$, then this behaviour is not deterministic anymore, but as $z_i>0$ still means that state $i$ will be on most of the time ($z_i > 0 \\rightarrow p(z_i)>0.5$), the system _tends_ to minimize its energy function.\n",
    "\n",
    "It is however hard, if not impossible, to easily say what configuration the system takes to assume. In the simple case where all biases are greater or equal to zero, and all connection weights are larger than zero, it is easy to see that a minimization of the energy is equivalent to a turning on of all neurons. Similarly the system tends to turn neurons off if biases and weights are all negative. In the case however where biases and/ or weights can be both negative and positive, the energy-minimizing configuration is not that easy to see.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the neuronal states of the network are updated in a way that is not dependent on the states, the network will eventually settle in a __Boltzmann distribution__. This means, that the occurrence probability $p(\\vec{s})$ of a state vector $\\vec{s}$ is dependent only to its energy:\n",
    "\n",
    "$$p(\\vec{s}) = \\frac{e^{-E(\\vec{s})}}{\\sum_\\vec{u}e^{-E(\\vec{u})}} \\ \\ \\ \\ \\ \\ (1)$$\n",
    "\n",
    "where $\\sum_\\vec{u}$ is the sum over all possible network states. Note that $p(\\vec{s})$ is proportional to the exponential of the state's negative energy, as $\\sum_\\vec{u}e^{-E(\\vec{u})} = const$. The distribution for $p(\\vec{s})$ derives its name from the similar-looking [Boltzmann Distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution) from statistical mechanics, which describes the probability of occurrence of a physical state in a thermodynamical system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "The learning rules for the connection strengths and biases of a Boltzmann machine without hidden units are$^{[1]}$:\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Learning of a Boltzmann Machine</b>\n",
    "$$\\Delta \\omega_{ij} = \\alpha_\\omega \\cdot \\left( \\langle s_i s_j \\rangle_{data} - \\langle s_i s_j \\rangle_{model} \\right)$$\n",
    "$$\\Delta b_i = \\alpha_b \\cdot \\left( \\langle s_j \\rangle_{data} - \\langle s_j \\rangle_{model} \\right)$$\n",
    "    </div>\n",
    "    \n",
    "Here $\\alpha_\\omega$ and $\\alpha_b$ are the learning rates, and _model_ stands for the Boltzmann machine. Expect for very simple networks, the value $\\langle s_i s_j \\rangle_{model}$ is not easily accessible. To compute its exact value, we would have to evaluate equation 1 (the \"Boltzmann probability\" of a state), where we sum over all possible state vectors in the denominator. As this number scales exponentially with the number of units in the Boltzmann machine, this quickly becomes intractable. For example, while there are $2^{10} = 1024$ different possible states with 10 units, for a mere 50 units, there are $2^{50} = 1,125,899,906,842,624$ different states. Therefore, the value $\\langle s_i s_j \\rangle_{model}$ is commonly approximated by letting the BM run for a while and counting the occurrences of the different state vectors.\n",
    "\n",
    "## Hebbian Learning\n",
    "\n",
    "The above learning rules are interesting not only for their simplicity, but also for the fact that they are reminiscent to the neurobiological learning rule of [_Hebbian Learning_](https://en.wikipedia.org/wiki/Hebbian_theory#Hebbian_learning_account_of_mirror_neurons). Hebbian Learning states, somewhat simplified: \"What fires together, wires together\" and means that neurons that tend to fire simultaneously will form connections to excite each other. While this certainly has its validity for artificial neural networks, it turns out that it is not quite so simple for biological neurons. Many neuroscientific experiments have established that Hebbian learning requires temporal asynchrony, i.e. neuron A has to consistenly fire before neuron B in order for the connection from A to B to get strengthened over time$^{[4]}$. This well-established concept in neuroscience is now known as [Spike-Timing Dependent Plasticity (STDP)](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] [Geoffrey E. Hinton (2007) Boltzmann machine. Scholarpedia, 2(5):1668.](http://www.scholarpedia.org/article/Boltzmann_machine)\n",
    "\n",
    "[2] [Wikipedia: Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine)\n",
    "\n",
    "[3] Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. \n",
    "Cognitive science, 9(1), 147-169.\n",
    "\n",
    "[4] [Spike-Timing Dependent Plasticity (STDP) (Wikipedia)](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Coincidence_detection_in_neurobiology\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Synaptic_tagging\n",
    "\n",
    "- https://www.mitpressjournals.org/doi/10.1162/neco.1995.7.5.889\n",
    "\n",
    "## Papers\n",
    "\n",
    "- [Hinton, G. E. (2012). A practical guide to training restricted Boltzmann machines. In Neural networks: Tricks of the trade (pp. 599-619). Springer, Berlin, Heidelberg.\n",
    "](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "\n",
    "- [Series, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model. In Advances in Neural Information Processing Systems (pp. 2020-2028).](https://homepages.inf.ed.ac.uk/pseries/pdfs/ReichertSeriesStorkeyNIPS.pdf)\n",
    "\n",
    "- [Bengio, Y., Lee, D. H., Bornschein, J., Mesnard, T., & Lin, Z. (2015). Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156.](https://arxiv.org/pdf/1502.04156.pdf)\n",
    "\n",
    "- [Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine. Neural computation, 7(5), 889-904.](http://www.gatsby.ucl.ac.uk/~dayan/papers/hm95.pdf)\n",
    "\n",
    "- [Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558.](https://www.pnas.org/content/pnas/79/8/2554.full.pdf)\n",
    "\n",
    "- [Hinton, G. E., & Sejnowski, T. J. (1983, June). Optimal perceptual inference. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (Vol. 448). IEEE New York.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.445.4504&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Approximation to the Boltzmann Distribution\n",
    "\n",
    "We want to check that the Boltzmann machine (BM) actually generates state vectors with their respective boltzmann-probabilities (equation (1)). This should be the case after the BM had the chance to settle into its stable state, thus after a certain number of iterations. The longer it iterates (i.e. generates data), the closer it should be to the Boltzmann distribution.\n",
    "\n",
    "In the following we will check whether the average distance between the boltzmann-probabilities of binary states with 10 entries and their probabilities of being generated by the (randomly initialised) BM actually decreases with more iterations.\n",
    "\n",
    "Note: The following is merely a proof of principle and not a strict mathematical proof the boltzmann property!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1484.71it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 2112.07it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 1310.83it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 1810.08it/s]\n",
      "100%|██████████| 125/125 [00:00<00:00, 2255.53it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 2491.11it/s]\n",
      "100%|██████████| 343/343 [00:00<00:00, 2244.51it/s]\n",
      "100%|██████████| 512/512 [00:00<00:00, 2420.86it/s]\n",
      "100%|██████████| 729/729 [00:00<00:00, 2445.25it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2420.84it/s]\n",
      "100%|██████████| 1331/1331 [00:00<00:00, 2464.08it/s]\n",
      "100%|██████████| 1728/1728 [00:00<00:00, 2475.04it/s]\n",
      "100%|██████████| 2197/2197 [00:00<00:00, 2307.15it/s]\n",
      "100%|██████████| 2744/2744 [00:01<00:00, 2448.73it/s]\n",
      "100%|██████████| 3375/3375 [00:01<00:00, 2416.63it/s]\n",
      "100%|██████████| 4096/4096 [00:01<00:00, 2360.40it/s]\n",
      "100%|██████████| 4913/4913 [00:02<00:00, 2449.89it/s]\n",
      "100%|██████████| 5832/5832 [00:02<00:00, 2476.71it/s]\n",
      "100%|██████████| 6859/6859 [00:02<00:00, 2430.02it/s]\n",
      "100%|██████████| 8000/8000 [00:03<00:00, 2450.41it/s]\n",
      "100%|██████████| 9261/9261 [00:03<00:00, 2435.78it/s]\n",
      "100%|██████████| 10648/10648 [00:04<00:00, 2418.51it/s]\n",
      "100%|██████████| 12167/12167 [00:05<00:00, 2389.20it/s]\n",
      "100%|██████████| 13824/13824 [00:05<00:00, 2357.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import BM #import Boltzmann Machine script (from this repository)\n",
    "import numpy as np\n",
    "N = 10 # There are 1024 different binary state vectors of length 10\n",
    "\n",
    "iterations = [i**3 for i in range(1,25)]\n",
    "distances = []\n",
    "\n",
    "for no_of_iterations in iterations:\n",
    "    bm = BM.BoltzmannM(N, initial_state=np.ones(N)) # Initialise BM (random weights and biases are automatically assigned)\n",
    "    \n",
    "    laststate, state_history = bm.iterate(no_of_iterations, savehist=True, suppress_output=True) # Iterate \n",
    "\n",
    "    import itertools\n",
    "    all_possible_states = [list(i) for i in itertools.product([0, 1], repeat=N)] # Create a list of all 1024 possible states\n",
    "    energies = bm.energy(all_possible_states) # Use the method energy() to compute the energy for each element in the list (Here an element is a list of length 10)\n",
    "\n",
    "    boltzmann_probs = []\n",
    "    for idx, state in enumerate(all_possible_states):\n",
    "        boltzmann_probability = np.exp(-energies[idx])/np.exp(-np.array(energies)).sum()\n",
    "        boltzmann_probs.append(boltzmann_probability)\n",
    "\n",
    "    # boltzmann_probs now contains the boltzmann-probabilities for the states in all_possible_states\n",
    "    # Compute the probability of a state actually being generated by the model\n",
    "\n",
    "    model_probs = []\n",
    "    for state in all_possible_states:\n",
    "        occurences_in_history = [1 for s in state_history if s == state]\n",
    "        model_probs.append(sum(occurences_in_history)/no_of_iterations)\n",
    "\n",
    "    # model_probs now contains the probabilities of occurrence for the states in state_history (generated by the BM)\n",
    "\n",
    "    # The average distance between the probabilities is thus \n",
    "    average_distance = (abs(np.array(boltzmann_probs) - np.array(model_probs))).mean()\n",
    "    distances.append(average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEKCAYAAAA7LB+5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5dnH8e/NTtj3PYR9B8GwiStqUXFD2qq1brwW21dtfdvK5oaCiktdaqtW6lKr1loCIohLqUutCyoKCYQ9LGHfIZCEbPf7x4xtpESCzORMZn6f65rrmpx5ZubOw+THM2fOucfcHRERiV1Vgi5ARES+nYJaRCTGKahFRGKcglpEJMYpqEVEYpyCWkQkximoRURinIJaRCTGKajjiJndZ2Y3x0Adn5lZr6DrOBLNkVRGCuo4YWbNgKuAP5Rj7DozO+sYHvuYxgMPAXcfw/gKoTmSykpBHePMrEU5h14DzHP3vCiWU16vA2eYWcugCznMNWiOIuoYXp9yHBTUMcjMGprZz8zsM+D5UtvXmdlEM8s0sz1m9pyZ1QrffC7wwWGPM97MNplZjpmtMLMzzezPQDIwx8wOmNm48NgJZrYmPDbTzEaFt5c1vrWZpZnZDjNba2Y///p53T0fWAiMiNYcHclR5gc0R9HwfHg3zk/NrGHQxcQtd9clBi6E/tP8HvAXYB8wC7gIqF5qzDpgCdAOaAx8BEwN37YDGFhqbDcgG2gd/jkF6FTqcc467Pl/ALQO13EpcBBodaTx4TELgTuAGkBHIAsYUWrMb4GHK3gOy5wfzVHU5rw6cHH49boPeBk4G6gSdG3xdNGKOgaY2Y2E/tCnAZ8QCotR7j7b3QsPG/47d892993APcDl4e0NgZxS44qBmkBPM6vu7uvcfU1ZNbj739x9s7uXuPtfgVXAoDKGDwSaufvd7l7g7lnAdOCyUmNywjV9/Tu+amYfhS//MLOapba/XWrcVDPbVlad5VDW/EDsz9GDZlbW41Fq3CNm1j98/QwzSz7afaLF3Qvd/TV3HwV0Aj4F7gfWhV/XEgEK6tjQAWgELAIWA7u+ZWx2qevrCa3wAPYA9b6+wd1XAzcDk4HtZvaKmbWmDGZ2lZktMrO9ZrYX6A00LWN4e6D112PD4ycBpfdX1gP2lvq5K3Cauw8jFFBfH/GQTCgwMbPGwFBg6RHquyK8W+GAmb1Z1u9B2fMDsT9HvTnC734EPYDM8PUxQKz0Kt4FpBN6HTci9LqWCFBQxwB3/xWh1cgS4HFgrZlNMbMuRxjertT1ZGBz+Ho6oTAs/bgvu/vJhELDCa104LA/bDNrT2i1dyPQxN0bhmuxI40nFIZr3b1hqUs9dz+v1JgehP7TwcxqEHorXGRmdYDmwOrw9kIgJ7wv+dfAWxwhrNz9JXevG76ce4R5Odr8QAzPUVhr4BEz+9LM/jf8vDeY2admtsDMBofH1XL3Q2Z2IXA+8Gczu9LM3g9fss3sRjP7wsx+Z2YrzexaM3vGzFaZ2chSv9cUM3vXzBab2bDwts/M7HEzW2pmN5W1rdRjdDGzKcBa4DEgA+gYfl1LBCioY4S7b3f3h929LzCa0FviT8zs2cOG3mBmbcOrz1uBv4a3zwNO+3qQmXUzs+HhXQz5QB5QEr55G6F9pl+rQyhodoTvey2h1R1ljP+MULiON7PaZlbVzHqb2cDw/WsBJwJ/D4/vDrQys/eBNcDz7r4/vH0lsAIYQmif8T5CAfhdlTU/EMNzZGZNCa1CxwMnAT8J7wY5mdC7jB8Ct5hZ8/BzAcwFFrr76e7+Z3c/HZgIvAG8CjQAJgA/An4F3ARcC5Q+jHCauw8HrgMuCdfRhNDq/1Tg3CNtKzWHzxLaXdcQuMTd+7n7I+6+A4kYBXUMcveF7n4ToRXWU4fd/DLwDqEPptYAU8PbXwDOM7Pa4Z9rEtrnvRPYSmgVOzF8233AbeG35L9290zgN4T+4LYBfQh9EEcZ44sJreROILSK2gn8kVAwAFwAvO/uX69mexPad3w6oXC+udT2pYTexv8OeJjQLpHyvP0vS1nzA7E9R32Bl9x9j4eOCMkl9CHdE+7uhP4DyQs/b0b4Pp0J7ScHIBzsY4AbCM3tX939AKEPVl9x99zw9XXh8c2Bp8zsPULvFjaF63jF3XMI7aZZV8a2rz1F6MPYm9z9SyQ6gv40U5fyXzjCkQiH3X4vcHMM1LkA6H1YXaPC1zsBH5XaPiK87f7wtvlAw2jMT4zP0c385wie0YRWwg8CZ4a33Q98PzzuovC2UV//LoRW588C1Uo93ujw9VuAkeHrtwNnhK8/DpwTvv4cMPyw+10OXH+kbUHPX6Jdqn2HbJcY5e6Tgq4BwN0HH7apNzAifBRAMfCzUtufdPdsQm/5IRTSe4mSGJ6jPkCumb0KHAB+Smgf+5/C+3//6e4zzOwZQifLQGiX0RQzSyF0uGAWMN/MFhH6oHJOeFw/Qu80vr7+RPj6R8BDZnYpoXcyi4ErDrvfLGDsEbZJBbLw/5JSCZjZOuA6d58fdC2xSPMj8UpBLSIS4/RhoohIjIvKPuqmTZt6SkpKNB5aRCQuLVy4cKe7NzvSbVEJ6pSUFL744otoPLSISFwys/Vl3aZdHyIiMU5BLSIS4xTUIiIxTkEtIhLjFNQiIjFOQS0iEuMU1CIiMU5BLSISAZ+v281TH5T5TW7HRd3zRESOw4FDRTzw1nJe+GQ9yY2TuGpoe5JqRDZaFdQiIt/R+yu2c+usJWzel8e1w1L49fe6RTykQUEtInLM9hwsYMobmcz8chOdm9dlxk9P4sT2jaL2fApqEZFycnfeXLKVO2YvYW9uITcN78yNwztTs1rVqD6vglpEpBy278/n9tlLeHvpNvq0acALYwbTs3X9CnluBbWIyLdwd/62cCNT52ZyqKiEied2539O7kC1qhV30JyCWkSkDNm7c5k4M4N/rd7JoA6NmXZJHzo2q1vhdSioRUQOU1zi/OnjdTz49gqqVjGmXtybHw1KpkoVC6QeBbWISCmrtuUwPi2dLzfs5fRuzbh3VB9aN6wdaE0KahERoLC4hKfeX8Pj766mTs2qPHrpCVx0QmvMgllFl6agFpGEl7FxH7fMWMzyrTlc0K81d17Qk6Z1awZd1r8pqEUkYeUXFvPI/JVM/2cWzerVZPpVqZzds0XQZf0XBbWIJKRPs3YxIS2ddbtyuXxQOyac24MGtasHXdYRKahFJKHk5Bcy7c3lvLRgA8mNk3j5usGc1Llp0GV9KwW1iCSM95ZvZ9KsDLbtz+e6kzvwy+91jUoTpUiL/QpFRI7T7oMF3D1nKa8t2kzXFnV54oqT6J8cvSZKkaagFpG45e7MSd/C5NeXkpNfyC/O7MINZ3SmRrXK9Z0pCmoRiUtb9+Vz22tLmL9sG/3aNuD+7w+me8uKaaIUaQpqEYkr7s4rn2dz7xvLKCwp4dbzejDm5A5UDej070goV1Cb2f8B1wEOZADXunt+NAsTETlW63cdZEJaBp9k7WJIx8ZMu6QvKU3rBF3WcTtqUJtZG+DnQE93zzOzV4HLgOejXJuISLkUlzjPfbSWh95ZQfUqVbjvkj5cNrBdTJz+HQnl3fVRDahtZoVAErA5eiWJiJTfiq05jEtLZ3H2Xs7q0ZypF/ehZYNaQZcVUUcNanffZGYPARuAPOAdd3/n8HFmNhYYC5CcnBzpOkVEvqGgqIQn3l/N799bTb1a1fnt5f25oG+ruFlFl1aeXR+NgIuADsBe4G9m9mN3f7H0OHd/GngaIDU11aNQq4gIAIuy9zJ+RjortuVw0QmtufOCXjSuUyPosqKmPLs+zgLWuvsOADObCZwEvPit9xIRibC8gmIe/vsKnvnXWprXq8UzV6dyZo/Ya6IUaeUJ6g3AEDNLIrTr40zgi6hWJSJymI/X7GRCWgYbdudyxeBkxp/bnfq1YrOJUqSVZx/1AjObAXwJFAFfEd7FISISbfvzC7lv3nL+8tkGUpok8crYIQzp2CTosipUuY76cPc7gTujXIuIyDfMz9zGra9lsCPnENef2pGbz+pK7RpVgy6rwunMRBGJObsOHGLynEzmLN5M95b1mH5VKn3bNgy6rMAoqEUkZrg7ry/ezOTXl3LgUBG/PLsrPz2tU6VrohRpCmoRiQmb9+Zx22tLeHf5dk5o15AHvt+Xri3qBV1WTFBQi0igSkqcv3y+gfvmLae4xLn9/J5cc1JKpW6iFGkKahEJzNqdB5mQls6CtbsZ1rkJ943qS3KTpKDLijkKahGpcEXFJTz70Vp+885KalSrwv2j+/DD1PhpohRpCmoRqVDLtuxnfFo66Rv3cXbPFky9uDct6sdXE6VIU1CLSIU4VFTM799dzRPvr6FhUnV+/6MBnNenpVbR5aCgFpGo+3LDHsbPSGfV9gNc0r8Nt5/fk0Zx3EQp0hTUIhI1uQVFPPT2Sp77eC2t6tfiuWsHcka35kGXVekoqEUkKj5avZMJM9PJ3p3HlUPaM+6cbtRLkCZKkaagFpGI2pdXyL1vLOOvX2TToWkd/jp2CIMTrIlSpCmoRSRi3lm6ldteW8KugwX89LRO3HxWF2pVT7wmSpGmoBaR47Yj5xCT5yzljfQt9GhVn2euHkiftg2CLituKKhF5Dtzd15btIm75mSSe6iYW0Z0Y+ypHaleNbGbKEWaglpEvpNNe/O4dVYG76/YwYDkUBOlzs3VRCkaFNQickxKSpyXFqxn2pvLKXG484KeXDVUTZSiSUEtIuWWteMAE9Iy+Gzdbk7p0pR7R/WhXWM1UYo2BbWIHFVRcQnTP1zLI/NXUqtaFR78fl++f2Jbnf5dQRTUIvKtlm7ex/i0dJZs2s85vVpy98W9aF5PTZQqkoJaRI4ov7CYx99dxVMfZNEoqQZPXjGAc/u0CrqshKSgFpH/snD9bsbNSGfNjoOMHtCW28/vQcMkNVEKioJaRP7t4KEiHnx7BX/6ZB2tG9TmT2MGcVrXZkGXlfAU1CICwD9X7mDizAw278vj6qEp3DKiG3VqKiJigf4VRBLc3twCpr6xjBkLN9KxWR3+dv1QUlMaB12WlKKgFklgb2Zs4fbZS9mTW8ANZ3TipuFqohSLFNQiCWh7Tj53zl7Km0u20qt1ff40ZiC9WquJUqxSUIskEHdnxsKNTH1jGXmFxYw/pzvXndJBTZRinIJaJEFk785l0qwMPly1k4EpjZg2ui+dmtUNuiwpBwW1SJwrKXFe+GQdD7y9AgOmXNSLKwa3p4qaKFUaCmqROLZ6ew7j0zJYuH4Pp3Vtxj2jetO2kZooVTYKapE4VFhcwtP/zOKx+atIqlmVh3/Yj1H926iJUiWloBaJM0s27eOWGeks27KfkX1bMfmCXjSrVzPosuQ4KKhF4kR+YTGPzl/F9A+zaFynBn+48kRG9GoZdFkSAQpqkTjw2drdTEhLJ2vnQS5Nbcek83rQIKl60GVJhJQrqM2sIfBHoDfgwBh3/ySahYnI0eXkF/LAWyv486frade4Ni/+z2BO7tI06LIkwsq7on4MeMvdv29mNQB9bCwSsPdWbOfWmRls2Z/PmGEd+PWIriTV0JvkeHTUf1UzawCcClwD4O4FQEF0yxKRsuw5WMCUuZnM/GoTXZrXJe1nJzEguVHQZUkUlee/3w7ADuA5M+sHLAR+4e4HSw8ys7HAWIDk5ORI1ymS8NydNzK2cOfspezLK+Tnwztzw/DO1KymJkrxrjwn+FcDBgBPunt/4CAw4fBB7v60u6e6e2qzZmo0LhJJ2/bnc/2fF3Ljy1/RplFt5tx0Mr/8XjeFdIIoz4p6I7DR3ReEf57BEYJaRCLP3Xn1i2ymvrGMgqISJp3XnTHDOlBNTZQSylGD2t23mlm2mXVz9xXAmUBm9EsTSWwbduUycVY6H63exeAOjbl/dF9SmtYJuiwJQHk/Ir4JeCl8xEcWcG30ShJJbMUlzvMfr+Oht1dQtYpxz6jeXD4wWU2UEli5gtrdFwGpUa5FJOGt3JbDuBnpLMrey/DuzblnVG9aNagddFkSMB10KRIDCopKeOqDNTz+7irq1qzGY5edwIX9WquJkgAKapHALc7ey/i0dJZvzeGCfq2ZfEFPmtRVEyX5DwW1SEDyCop5dP5Kpn+YRbN6NZl+VSpn92wRdFkSgxTUIgH4NGsXE9LSWbcrl8sHJTPxvO7Ur6UmSnJkCmqRCpSTX8i0N5fz0oINtG+SxMs/GcxJndRESb6dglqkgry7fBu3zlrCtv35/OSUDvzy7G7UrqEzC+XoFNQiUbbrwCHunpvJ7EWb6daiHk/++EROaNcw6LKkElFQi0SJuzMnfQuTX19KTn4hN5/Vhf89vTM1qun0bzk2CmqRKNi6L5/bXstg/rLt9GvXkAdG96Vby3pBlyWVlIJaJILcnVc+z+beN5ZRWFLCbSN7cO2wDlTV6d9yHBTUIhGyftdBJqRl8EnWLoZ2bMK00X1o30RNlOT4KahFjlNxifPcR2t56J0VVK9Shfsu6cNlA9vp9G+JGAW1yHFYsTWHcWnpLM7ey1k9mjP14j60bFAr6LIkziioRb6DgqISfv/eap54fzX1a1Xn8cv7c37fVlpFS1QoqEWO0aLsvYybsZiV2w5w8QmtueOCXjSuUyPosiSOKahFyimvoJjfvLOCZz9aS4v6tXj2mlSGd1cTJYk+BbVIOXy8ZicT0jLYsDuXHw1OZuK53amnJkpSQRTUIt9if34h981bxl8+yyalSRKvjB3CkI5Ngi5LEoyCWqQM8zO3cetrGezIOcT1p3bk5rO6qomSBEJBLXKYnQcOcdecTOYs3kz3lvWYflUqfduqiZIER0EtEubuzF60mbvmLOXgoWJ+dXZXrj+tk5ooSeAU1CLA5r153PbaEt5dvp3+yaEmSl1aqImSxAYFtSS0khLn5c82MO3N5RSXOHec35OrT0pREyWJKQpqSVhrdx5kQlo6C9bu5uTOTbnvkj60a5wUdFki/0VBLQmnqLiEZ/61lof/vpIa1arwwOi+/CC1rU7/lpiloJaEsmzLfsanpZO+cR/f69mCKRf3pkV9NVGS2KagloRwqKiY3727miffX0PDpOr8/kcDOK9PS62ipVJQUEvcW7h+D+PT0lm9/QCXDGjD7SN70khNlKQSUVBL3MotKOLBt1fw/MfraFW/Fs9dO5AzujUPuiyRY6aglrj0r1U7mTAznY178rhqaHvGndOdujX1cpfKSa9ciSv7cgu5Z14mr36xkY5N6/Dq9UMZ1KFx0GWJHBcFtcSNt5Zs5fbZS9h9sICfnd6JX5zZhVrV1URJKj8FtVR6O3IOMfn1pbyRsYWererz3DUD6d2mQdBliUSMgloqLXdn5pebuHtuJnkFxdwyohtjT+1I9apqoiTxRUEtldKmvXlMmpnBByt3cGL7Rtw/ui+dm9cNuiyRqCh3UJtZVeALYJO7nx+9kkTKVlLivLhgPfe/uRwH7rqwF1cOaU8VNVGSOHYsK+pfAMuA+lGqReRbrdlxgAlp6Xy+bg+ndGnKvaPUREkSQ7mC2szaAiOBe4BfRrUikcMUFpcw/cMsHp2/itrVq/LQD/oxekAbnf4tCaO8K+pHgXFAmZ3UzWwsMBYgOTn5+CsTAZZs2sf4tHSWbt7Pub1bctdFvWheT02UJLEcNajN7Hxgu7svNLPTyxrn7k8DTwOkpqZ6xCqUhJRfWMzj767iqQ+yaJRUgyevGMC5fVoFXZZIIMqzoh4GXGhm5wG1gPpm9qK7/zi6pUmi+mLdbsalpZO14yA/OLEtt47sQcMkNVGSxHXUoHb3icBEgPCK+tcKaYmGA4eKePCt5bzw6XpaN6jNC2MGcWrXZkGXJRI4HUctMeGDlTuYNDODzfvyuHpoCreM6EYdNVESAY4xqN39feD9qFQiCWlvbgFT5i4j7cuNdGpWh79dP5TUFDVREilNSxYJzJsZW7h99lL25BZw4xmduXF4ZzVREjkCBbVUuO3787lj9lLeWrqVXq3r86cxA+nVWk2URMqioJYK4+78beFGps7NJL+ohPHndOcnp3SgmpooiXwrBbVUiOzduUyalcGHq3YyKKUx00b3oWMzNVESKQ8FtURVcYnzwifrePDtFRgw5aJeXDFYTZREjoWCWqJm9fYcxqdlsHD9Hk7r2ox7L+lDm4a1gy5LpNJRUEvEFRaX8IcP1vDbf6wmqWZVHv5hP0b1VxMlke9KQS0RlbFxH7fMWMzyrTmM7NuKyRf0olm9mkGXJVKpKaglIvILi3l0/iqmf5hFkzo1+MOVJzKiV8ugyxKJCwpqOW4LsnYxYWYGa3ce5NLUdkwa2YMGtasHXZZI3FBQy3eWk1/IA2+t4M+frqdd49q8dN1ghnVuGnRZInFHQS3fyXsrtnPrzAy27M9nzLAO/HpEV5Jq6OUkEg36y5JjsvtgAVPmZjLrq010aV6XtJ+dxIDkRkGXJRLXFNRSLu7OGxlbuHP2UvblFfLzM7twwxmdqFlNTZREok1BLUe1bX8+t722hL9nbqNv2wa8eN1gerTSl9GLVBQFtZTJ3Xn1i2ymvrGMgqISJp3XnTHD1ERJpKIpqOWINuzKZcLMdD5es4vBHRpz/+i+pDStE3RZIglJQS3fUFziPP/xOh56ewVVqxhTL+7NjwYlq4mSSIAU1PJvK7flMG5GOouy93JGt2bcM6oPrdVESSRwCmqhoKiEJ99fw+/eW0XdmtV47LITuLBfazVREokRCuoEtzh7L+PT0lm+NYcL+rVm8gU9aVJXTZREYomCOkHlFRTzyPyV/PHDLJrVq8n0q1I5u2eLoMsSkSNQUCegT9bsYuLMdNbtyuXyQe2YeF4P6tdSEyWRWKWgTiD78wuZ9uZyXl6wgeTGSbx83WBOUhMlkZinoE4Q/1i2jVtnLWF7Tj4/OaUDvzy7G7Vr6PRvkcpAQR3ndh04xF1zMnl98Wa6tajHU1eeyAntGgZdlogcAwV1nHJ3Xl+8mbvmZJKTX8jNZ3Xhf0/vTI1qOv1bpLJRUMehLfvyuG3WEv6xfDv92jXkgdF96dayXtBlich3pKCOIyUlziufZ3PfvGUUlpRw28geXDusA1V1+rdIpaagjhPrdh5kwsx0Ps3azdCOTZg2ug/tm6iJkkg8UFBXckXFJTz70Vp+885KalStwrRL+nDpwHY6/VskjiioK7HlW/czfkY6izfu46wezZl6cR9aNqgVdFkiEmEK6kroUFExv39vDU+8t5oGtavz+OX9Ob9vK62iReKUgrqS+WrDHsanpbNy2wEuPqE1d1zQi8Z1agRdlohEkYK6ksgtKOI376zk2Y/W0rJ+LZ69JpXh3dVESSQRHDWozawd8ALQAnDgaXd/LNqFyX98vHonE2ZmsGF3Lj8eksz4c7pTT02URBJGeVbURcCv3P1LM6sHLDSzv7t7ZpRrS3j78gq5b94yXvk8m5QmSbwydghDOjYJuiwRqWBHDWp33wJsCV/PMbNlQBtAQR1Ff8/cxm2vZbAj5xDXn9aR/zurK7Wqq4mSSCI6pn3UZpYC9AcWHOG2scBYgOTk5AiUlph2HjjE5NeXMjd9C91b1mP6Van0basmSiKJrNxBbWZ1gTTgZnfff/jt7v408DRAamqqR6zCBOHuvLZoE3fNyST3UDG/Orsr15/WSU2URKR8QW1m1QmF9EvuPjO6JSWezXvzuHVWBu+t2EH/5FATpS4t1ERJRELKc9SHAc8Ay9z94eiXlDhKSpyXPtvA/W8up7jEueP8nlx9UoqaKInIN5RnRT0MuBLIMLNF4W2T3H1e9MqKf1k7DjAhLYPP1u3m5M5Nue+SPrRrnBR0WSISg8pz1Me/AC3xIqSouIQ//mstj/x9JTWqVeGB0X35QWpbnf4tImXSmYkVKHPzfsalLWbJpv18r2cLplzcmxb11URJRL6dgroCHCoq5nfvrubJ99fQMKk6T1wxgHN7t9QqWkTKRUEdZQvXh5oord5+gEsGtOH2kT1ppCZKInIMFNRRcvBQEQ+9s4LnP15H6wa1ef7agZzerXnQZYlIJaSgjoIPV+1g4swMNu7J46qh7Rl3Tnfq1tRUi8h3o/SIoH25hdwzL5NXv9hIx6Z1ePX6oQzq0DjoskSkklNQR8hbS7Zy++wl7D5YwM9O78QvzuyiJkoiEhEK6uO0PSefya8vZV7GVnq2qs9z1wykd5sGQZclInFEQf0duTszv9zE3XMzySss5pYR3Rh7akeqV1UTJRGJLAX1d7BxTy6TZi3hnyt3cGL7Rtw/ui+dm9cNuiwRiVMK6mNQUuK8uGA997+5HAfuurAXVw5pTxU1URKRKFJQl9OaHQeYkJbO5+v2cEqXptw7Sk2URKRiKKiPorC4hOkfZvHo/FXUrl6Vh37Qj9ED2uj0bxGpMArqb7Fk0z7Gp6WzdPN+zuvTkskX9qJ5PTVREpGKpaA+gvzCYn77j1X84Z9ZNEqqwVM/HsA5vVsFXZaIJCgF9WG+WLebcWnpZO04yA9ObMttI3vSIKl60GWJSAJTUIcdOFTEg28t54VP19O6QW1eGDOIU7s2C7osEREFNcAHK3cwaWYGm/flcfXQFG4Z0Y06aqIkIjEiodNob24BU+YuI+3LjXRqVocZPx3Kie3VRElEYkvCBvW8jC3cMXsJe3MLufGMztw4vLOaKIlITEq4oN6+P587Zi/lraVb6d2mPn8aM4herdVESURiV8IEtbvzt4UbmTo3k/yiEsaf052fnNKBamqiJCIxLiGCOnt3LpNmZfDhqp0MSmnMtNF96NhMTZREpHKI66AuLnFe+GQdD769AgOmXNSLKwariZKIVC5xG9Srt+cwbkY6X27Yy+ndmnHPqD60aVg76LJERI5Z3AV1YXEJf/hgDb/9x2qSalblkUv7cfEJaqIkIpVXXAV1xsZ93DJjMcu35jCybyvuurAXTevWDLosEZHjEhdBnV9YzKPzVzH9wyya1KnBH648kRG9WgZdlohIRFT6oF6QtYsJMzNYu/Mgl6a2Y9LIHjSorSZKIhI/Km1Q5+QXcv9by3nx0w20a1ybl64bzLDOTYMuS0Qk4iplUL+3fDu3zspgy/58/ufkDvzqe11Jqt75+MwAAAR2SURBVFEpfxURkaOqVOm2+2ABU+ZmMuurTXRpXpe0n53EgORGQZclIhJVlSKo3Z256VuY/PpS9uUV8vMzu3DDGZ2oWU1NlEQk/sV8UG/bn8+ts5Ywf9k2+rZtwIvXDaZHq/pBlyUiUmFiNqjdnb9+ns0985ZRUFTCpPO6M2aYmiiJSOIpV1Cb2TnAY0BV4I/uPi2aRW3YlcuEmel8vGYXgzs05v7RfUlpWieaTykiErOOGtRmVhX4PXA2sBH43Mxed/fMSBdTXOI899FaHnpnBdWqVOGeUb25fGCymiiJSEIrz4p6ELDa3bMAzOwV4CIgokG9L7eQq5/7jEXZexnevTn3jOpNqwZqoiQiUp6gbgNkl/p5IzD48EFmNhYYC5CcnHzMhdSvXY32TZK4dlgKF/ZrrSZKIiJhEfsw0d2fBp4GSE1N9WO9v5nx2GX9I1WOiEjcKM8hFJuAdqV+bhveJiIiFaA8Qf050MXMOphZDeAy4PXoliUiIl876q4Pdy8ysxuBtwkdnvesuy+NemUiIgKUcx+1u88D5kW5FhEROQKd5iciEuMU1CIiMU5BLSIS4xTUIiIxztyP+dyUoz+o2Q5g/Xe8e1NgZwTLqcw0F9+k+fgmzcd/xMNctHf3Zke6ISpBfTzM7At3Tw26jligufgmzcc3aT7+I97nQrs+RERinIJaRCTGxWJQPx10ATFEc/FNmo9v0nz8R1zPRcztoxYRkW+KxRW1iIiUoqAWEYlxMRPUZnaOma0ws9VmNiHoeoJkZu3M7D0zyzSzpWb2i6BrCpqZVTWzr8xsbtC1BM3MGprZDDNbbmbLzGxo0DUFycz+L/x3ssTM/mJmtYKuKdJiIqhLfYHuuUBP4HIz6xlsVYEqAn7l7j2BIcANCT4fAL8AlgVdRIx4DHjL3bsD/UjgeTGzNsDPgVR3702oFfNlwVYVeTER1JT6Al13LwC+/gLdhOTuW9z9y/D1HEJ/iG2CrSo4ZtYWGAn8MehagmZmDYBTgWcA3L3A3fcGW1XgqgG1zawakARsDrieiIuVoD7SF+gmbDCVZmYpQH9gQbCVBOpRYBxQEnQhMaADsAN4Lrwr6I9mVifoooLi7puAh4ANwBZgn7u/E2xVkRcrQS1HYGZ1gTTgZnffH3Q9QTCz84Ht7r4w6FpiRDVgAPCku/cHDgIJ+5mOmTUi9O67A9AaqGNmPw62qsiLlaDWF+gexsyqEwrpl9x9ZtD1BGgYcKGZrSO0S2y4mb0YbEmB2ghsdPev32HNIBTcieosYK2773D3QmAmcFLANUVcrAS1vkC3FDMzQvsgl7n7w0HXEyR3n+jubd09hdDr4l13j7sVU3m5+1Yg28y6hTedCWQGWFLQNgBDzCwp/HdzJnH44Wq5vjMx2vQFuv9lGHAlkGFmi8LbJoW/u1LkJuCl8KImC7g24HoC4+4LzGwG8CWho6W+Ig5PJ9cp5CIiMS5Wdn2IiEgZFNQiIjFOQS0iEuMU1CIiMU5BLSIS4xTUIiIxTkEtIhLj/h/LYyq82bhPDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(iterations,distances)\n",
    "plt.title('<p(state)$_{BM}$ - p(state)$_{boltzmann}$>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Boltzmann Distribution\n",
    "\n",
    "Here I want to derive the fact that a Boltzmann machine creates a state $\\vec{s}$ with the Boltzmann-probability (equation 1).\n",
    "\n",
    "Remember the probability given that unit $i$ is turned on:\n",
    "\n",
    "$$p(s_i=1)=\\frac{1}{1+e^{-z_i}},$$\n",
    "\n",
    "and that $z_i$ is the energy contribution to the state of unit $i$ when turned on against being turned off, thus\n",
    "\n",
    "$$z_i = \\Delta E_i.$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$p(s_i=1)=\\frac{1}{1+e^{-\\Delta E_i}}.$$\n",
    "\n",
    "Now, imagine a Boltzmann machine with four units. We are interested in the state $\\vec{s}_{1,2}$ where units 1 and 2 are on, and units 3 and 4 are off. The probability of that state is given by the product of the different units being in the required state:\n",
    "\n",
    "$$p(\\vec{s}_{1,2})=\\frac{1}{1+e^{-\\Delta E_1}}\\cdot \\frac{1}{1+e^{-\\Delta E_2}} \\cdot \\left( 1- \\frac{1}{1+e^{-\\Delta E_3}}\\right)\\cdot \\left( 1- \\frac{1}{1+e^{-\\Delta E_4}}\\right).$$\n",
    "\n",
    "For the next step, we use the fact that $\\frac{1}{1+e^{-z_i}}=1-\\frac{1}{1+e^{z_i}}$, thus:\n",
    "\n",
    "$$p(\\vec{s}_{1,2})=\\left( 1- \\frac{1}{1+e^{\\Delta E_1}}\\right)\\cdot \\left( 1- \\frac{1}{1+e^{\\Delta E_2}}\\right)\\cdot \\frac{1}{1+e^{\\Delta E_3}}\\cdot \\frac{1}{1+e^{\\Delta E_4}}$$\n",
    "$$= \\frac{e^{\\Delta E_1}}{1+e^{\\Delta E_1}}\\cdot \\frac{e^{\\Delta E_2}}{1+e^{\\Delta E_2}}\\cdot \\frac{1}{1+e^{\\Delta E_3}}\\cdot \\frac{1}{1+e^{\\Delta E_4}}.$$\n",
    "$$=\\frac{e^{\\Delta E_1 + \\Delta E_2}}{\\prod_{i=1}^4 (1+e^{\\Delta E_i})}$$\n",
    "\n",
    "We're almost there. Remember that, when unit i is turned on, the system's energy _decreases_ by $\\Delta E_i$, thus $E=-\\sum_i E_i$. To see the identity of the denominator, we substitute $e^{\\Delta E_1} = a$, $e^{\\Delta E_2} = b$, $e^{\\Delta E_3} = c$, and $e^{\\Delta E_4} = d$. Then, the denominator is:\n",
    "\n",
    "$$(1+a)(1+b)(1+c)(1+d) = a b c d + a b c + a b d + a b + a c d + a c + a d + a + b c d + b c + b d + b + c d + c + d + 1.$$\n",
    "\n",
    "As you can see, the result contains every possible combination of products between a, b, c, and d of length four and less. Resusbtituting the first term of that line: $abcd = e^{\\Delta E_1+ \\Delta E_2+\\Delta E_3+\\Delta E_4}$, where $\\Delta E_1+ \\Delta E_2+\\Delta E_3+\\Delta E_4$ is the negative of the energy of the system when all four units are on. Thus, the denominator is a linear combination of the negative exponentials of the energies of all the possible states of the Boltzmann machine. The probably for the state we started with thus works out to be:\n",
    "\n",
    "$$p(\\vec{s}_{1,2}) = \\frac{e^{E(\\vec{s}_{1,2})}}{\\sum_\\vec{u} e^{-E(\\vec{u})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Learning Rules\n",
    "The learning rules can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle \\frac{\\partial}{\\partial \\omega_{ij}} \\ln p(\\vec{v})\\rangle_{data} = \\langle \\frac{\\partial}{\\partial \\omega_{ij}}\\left( -E(\\vec{v}) -\\ln \\sum_{\\vec{u}} e^{-E(\\vec{u})} \\right) \\rangle_{data} \\ \\ \\ \\ \\ \\ (2)\n",
    "\\end{align}\n",
    "$$\n",
    "Here, we used the identity of Equation (1) above. $\\langle \\cdot \\rangle_{data}$ stands for the expected value under the distribution of state vectors $\\vec{v}$ in the data. We can thus rewrite the above equation as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\vec{v}} p_{data}(\\vec{v})\\left( \\frac{\\partial}{\\partial \\omega_{ij}}\\left( -E(\\vec{v}) -\\ln \\sum_{\\vec{u}} e^{-E(\\vec{u})} \\right) \\right)  \\ \\ \\ \\ \\ \\ (3)\n",
    "\\end{align}\n",
    "$$\n",
    "In the previous equation $p_{data}(\\vec{v})$ stands for the probability of state vector $\\vec{v}$ occurring in the training data and must therefore not be confused with the boltzmann distribution in equation (1) above.\n",
    "\n",
    "For the next step we use the fact that $\\frac{\\partial}{\\partial \\omega_{ij}}E(\\vec{v})=-s_is_j$. At the same time we use $\\frac{\\partial}{\\partial \\omega_{ij}} \\ln \\sum_{\\vec{u}}e^{-E(\\vec{u})} = \\frac{\\frac{\\partial}{\\partial \\omega_{ij}} \\sum_{\\vec{u}}e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}$, because of $\\frac{\\partial}{\\partial x} \\ln(f(x)) = \\frac{\\frac{\\partial}{\\partial x} f(x)}{f(x)}$. Thus, equation (3) can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\vec{v}} p_{data}(\\vec{v})(s_is_j)_\\vec{v} - \\sum_{\\vec{v}} p_{data}(\\vec{v}) \\frac{\\frac{\\partial}{\\partial \\omega_{ij}} \\sum_{\\vec{u}}e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  \\ \\ \\ \\ \\ \\ (4).\n",
    "\\end{align}\n",
    "$$\n",
    "Now this is starting to look a little complicated, but upon closer inspection we see that the left hand side is simply the expectation value of $s_is_j$ under the state-distribution of the data, and the right-hand side is independent of the vector $\\vec{v}$ we started with. Therefore, the sum on the RHS just sums to one and equation (4) can be rewritten as (writing the derivative inside the sum):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle s_is_j \\rangle_{data} - \\frac{\\sum_{\\vec{u}} \\frac{\\partial}{\\partial \\omega_{ij}} e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  \\ \\ \\ \\ \\ \\ (5).\n",
    "\\end{align}\n",
    "$$\n",
    "Derivation as before gives:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle s_is_j \\rangle_{data} - \\frac{\\sum_{\\vec{u}} (s_is_j)_\\vec{u}\\cdot e^{-E(\\vec{u})}}{\\sum_{\\vec{u}}e^{-E(\\vec{u})}}  = \\langle s_is_j \\rangle_{data} - \\sum_\\vec{u}\\left( p(\\vec{u})\\cdot(s_is_j)_\\vec{u}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last step we used the definition of the Boltzmann distribution from euqation (1). As the distribution describes the distribution of state vectors of the Boltzmann machine (which is the _model_), the LHS of equation resolves to:\n",
    "\n",
    "$$\\langle \\frac{\\partial}{\\partial \\omega_{ij}} \\ln p(\\vec{v})\\rangle_{data} = \\langle s_is_j \\rangle_{data} - \\langle s_is_j \\rangle_{model}.$$\n",
    "\n",
    "The learning rule for $b_i$ can be derived analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "- Implement learning with hidden units\n",
    "\n",
    "- Bayesian properties?\n",
    "\n",
    "- Notebook: Hopfield networks\n",
    "\n",
    "- Read: Completion of incomplete data\n",
    "\n",
    "- Notebook: compare with Kennet (2003)\n",
    "\n",
    "- Restricted Boltzmann machines\n",
    "\n",
    "- What does this have to do with Helmholtz machine?\n",
    "\n",
    "\n",
    "# To Solve\n",
    "\n",
    "- (On-line learning algorithms?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
